clear

#!/bin/bash
# Define color variables

BLACK=`tput setaf 0`
RED=`tput setaf 1`
GREEN=`tput setaf 2`
YELLOW=`tput setaf 3`
BLUE=`tput setaf 4`
MAGENTA=`tput setaf 5`
CYAN=`tput setaf 6`
WHITE=`tput setaf 7`

BG_BLACK=`tput setab 0`
BG_RED=`tput setab 1`
BG_GREEN=`tput setab 2`
BG_YELLOW=`tput setab 3`
BG_BLUE=`tput setab 4`
BG_MAGENTA=`tput setab 5`
BG_CYAN=`tput setab 6`
BG_WHITE=`tput setab 7`

BOLD=`tput bold`
RESET=`tput sgr0`

# Array of color codes excluding black and white
TEXT_COLORS=($RED $GREEN $YELLOW $BLUE $MAGENTA $CYAN)
BG_COLORS=($BG_RED $BG_GREEN $BG_YELLOW $BG_BLUE $BG_MAGENTA $BG_CYAN)

# Pick random colors
RANDOM_TEXT_COLOR=${TEXT_COLORS[$RANDOM % ${#TEXT_COLORS[@]}]}
RANDOM_BG_COLOR=${BG_COLORS[$RANDOM % ${#BG_COLORS[@]}]}

# Function to fetch the table ID and format it with the desired output pattern
fetch_table_id() {
    export project_id=$DEVSHELL_PROJECT_ID
    export dataset_id="project_logs"
    export formatted_output=""

    while true; do
        # Fetch the table ID from BigQuery
        export table_id=$(bq ls --project_id "$project_id" --dataset_id "$dataset_id" --format=json \
            | jq -r '.[0].tableReference.tableId')

        # Check if table_id is empty
        if [[ -n "$table_id" ]]; then
            # Format the output as desired
            formatted_output="$project_id.$dataset_id.${table_id}"
            break
        fi

        echo "Waiting for table to be available..."
        sleep 5
    done
}

#----------------------------------------------------start--------------------------------------------------#

echo "${RANDOM_BG_COLOR}${RANDOM_TEXT_COLOR}${BOLD}Starting Execution${RESET}"

# Step 1: Set the default zone from project metadata
echo -e "${GREEN}${BOLD}Setting the default zone from project metadata...${RESET}"
export ZONE=$(gcloud compute project-info describe \
--format="value(commonInstanceMetadata.items[google-compute-default-zone])")

# Step 2: Fetching Project ID and Project Number
echo -e "${YELLOW}${BOLD}Fetching Project ID and Project Number...${RESET}"
export PROJECT_ID=$(gcloud config get-value project)
export PROJECT_NUMBER=$(gcloud projects describe ${PROJECT_ID} \
    --format="value(projectNumber)")

mkdir stackdriver-lab
cd stackdriver-lab

# Step 3: Download necessary files
echo -e "${MAGENTA}${BOLD}Downloading necessary files for the lab...${RESET}"
FILES=( 
    "activity.sh"
    "apache2.conf"
    "basic-ingress.yaml"
    "gke.sh"
    "linux_startup.sh"
    "pubsub.sh"
    "setup.sh"
    "sql.sh"
    "windows_startup.ps1"
)

BASE_URL="https://github.com/QUICK-GCP-LAB/2-Minutes-Labs-Solutions/raw/refs/heads/main/Configuring%20and%20Using%20Cloud%20Logging%20and%20Cloud%20Monitoring/stackdriver-lab"

for file in "${FILES[@]}"; do
    curl -LO "${BASE_URL}/${file}"
done

# Step 4: Update setup.sh with the current zone
echo -e "${CYAN}${BOLD}Updating 'setup.sh' with the current zone...${RESET}"
sed -i "s/us-west1-b/$ZONE/g" setup.sh

# Step 5: Make necessary scripts executable and run setup
echo -e "${RED}${BOLD}Making scripts executable and running 'setup.sh'...${RESET}"
chmod +x *.sh
./setup.sh

# Step 6: Create a BigQuery dataset for logs
echo -e "${GREEN}${BOLD}Creating a BigQuery dataset named 'project_logs'...${RESET}"
bq mk project_logs

# Step 7: Create logging sinks for VM and Load Balancer logs
echo -e "${YELLOW}${BOLD}Creating logging sinks for VM and Load Balancer logs...${RESET}"
gcloud logging sinks create vm_logs \
    bigquery.googleapis.com/projects/$DEVSHELL_PROJECT_ID/datasets/project_logs \
    --log-filter='resource.type="gce_instance"'

gcloud logging sinks create load_bal_logs \
    bigquery.googleapis.com/projects/$DEVSHELL_PROJECT_ID/datasets/project_logs \
    --log-filter="resource.type=\"http_load_balancer\""

# Step 8: Add IAM policy for the service account
echo -e "${BLUE}${BOLD}Adding IAM policy for the service account...${RESET}"
gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID \
  --member=serviceAccount:service-$PROJECT_NUMBER@gcp-sa-logging.iam.gserviceaccount.com \
  --role=roles/bigquery.dataEditor

# Step 9: Fetch the table ID from the BigQuery dataset
echo -e "${CYAN}${BOLD}Fetching the table ID from the 'project_logs' dataset...${RESET}"
fetch_table_id
echo "Table Id: $formatted_output"

# Step 11: Create a logging metric for 403 errors
echo -e "${GREEN}${BOLD}Creating a logging metric for 403 errors...${RESET}"
gcloud logging metrics create 403s \
    --description="Counts syslog entries with resource.type=gce_instance" \
    --log-filter="resource.type=\"gce_instance\" AND logName=\"projects/$DEVSHELL_PROJECT_ID/logs/syslog\""

# Step 12: Display BigQuery query template
echo
echo -e "${BLUE}${BOLD}Go to: https://console.cloud.google.com/bigquery?project=$DEVSHELL_PROJECT_ID${RESET}"
echo
echo -e "Copy and run the query below:\n"
echo -e "${CYAN}${BOLD}SELECT logName, resource.type, resource.labels.zone, resource.labels.project_id FROM \`$formatted_output\`${RESET}"

echo

# Function to display a random congratulatory message
function random_congrats() {
    MESSAGES=(
        "${GREEN}Congratulations For Completing The Lab! ${RESET}"
            )

    RANDOM_INDEX=$((RANDOM % ${#MESSAGES[@]}))
    echo -e "${BOLD}${MESSAGES[$RANDOM_INDEX]}"
}

# Display a random congratulatory message
random_congrats

echo -e "\n"  # Adding one blank line

cd

remove_files() {
    # Loop through all files in the current directory
    for file in *; do
        # Check if the file name starts with "gsp", "arc", or "shell"
        if [[ "$file" == gsp* || "$file" == arc* || "$file" == shell* ]]; then
            # Check if it's a regular file (not a directory)
            if [[ -f "$file" ]]; then
                # Remove the file and echo the file name
                rm "$file"
                echo "File removed: $file"
            fi
        fi
    done
}

remove_files
